---
title: "Modeling of Smoking-Related Mortality: A Big-Data Analysis of the NLMS (National Longitudinal Mortality Study) Tobacco-Use Cohort in the U.S."
author: "Naman Dariwhal, Nicole Jerome, Indhira Vadivel"
date: "12-12-2025"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(dplyr)
```

# Abstract


# Introduction

# Methods
### Data Acquisition and Description
The National Longitudinal Mortality Study (NLMS) Public Use Microdata Sample (PUMS) Tobacco use cohort (dataset file name: tu) contains 493,282 participants who were tracked over a five year period. Forty-three different variables including demographic information, socioeconomic factors, and smoking behaviors were collected for each individual. Importantly, the death indicator column (inddea) reported whether an individual was alive or deceased at the end of the follow up period. Data was acquired from _____ and was uploaded to GitHub for our analyses. 

### Exploratory Data Analysis (EDA) and Logistic Regression
#### 1. Workflow and Data Processing
A big-data workflow centered on Apache Arrow enabled efficient, memory-safe processing of the large dataset. This allowed fast columnar input/output, lazy evaluation, and on-disk querying without importing the entire file into memory—- an approach that saved memory and computing costs over base R techniques. The raw CSV file (tu) was converted to a Parquet file, and was loaded as an Arrow dataset. Arrow and dplyr were used to subset the necessary variables for recoding and perform grouped operations. Smokeless tobacco use variables (curruse, everuse) were recoded into binary indicators and marital status was collapsed into two categories (married, not married/separated). Labeled factor variables were also created for readability.

#### 2. Exploratory Data Analysis
Descriptive statistics were computed, stratified by sex, with categorical variables being summarized by counts or percentages, and continuous variables being summarized by means and standard deviations. Table 1 was generated using kableExtra to present all variables with clear labels and hierarchical indentation.

#### 3. Variable Selection

#### 4. Logistic Regression Modeling


### Machine Learning Predictor
#### 1. Data Preprocessing
Within tu, the outcome variable (death indicator, inddea) was already coded as a binary predictor of death, however, each predictor needed to be assessed for its usefulness in the model. In total, 17 out of 43 variables were able to be used within the model. Variables that were redundant (ex: occupation code and industry code), irrelevant (ex: record number, presence of social security number), strongly subjective (ex: health status), confounded with the death indicator (ex: cause of death), contained a high percentage of missing data, or that were present but did not contain data (ex: standard metropolitan statistical area (smsa) status)(the tu file is a subset of a larger study and thus some identification data was not included), were eliminated. As such, the predictors included:

**Demographics:** age (age), sex (sex), race/ethnicity (race), country of birth (pob), region of residence (stater)  
**Socioeconomic:** income as percent of poverty ratio (povpct), education (educ), employment status (esr), occupation (majocc), citizenship (citizen)  
**Household/Social:** marital status (ms), number of people living in household (hhnum), veteran status (vt)  
**Health-Related:** health insurance coverage status (histatus), smoking status (smokstat), smoked more than 100 cigarettes in lifetime (smok100), smokeless tobacco use (everuse) 

Categorical predictors (ex: marital status, employment, region, smoking status) were converted into machine-readable form using one-hot encoding. Binary variables remained as 0/1, and continuous variables (ex: age, povpct) were not altered.

#### 2. Handling Class Imbalance
In tu, deaths account for only ~4% of observations (Alive	(0): 378,501; Deceased (1):	16,124). Therefore, a naive classifier model would achieve ~96% accuracy by predicting survival for everyone. If unaltered, the accuracy of the model would be misleading. This highlights the importance of additional metrics such as recall, balanced accuracy, ROC-AUC, and PR-AUC. To correct for the imbalance, LightGBM’s scale_pos_weight parameter was set to: scale_pos_weight = (# survivors / # deaths). This setting increases the penalty for misclassifying deaths and helps the model focus on the minority class without oversampling or synthetic data generation. A stratified 80/20 training/testing split ensured proportional representation of deaths in both data sets.

#### 3. Machine Learning Modeling Methods
The Light Gradient Boosting Machine (LightGBM) [1] framework was chosen as a machine learning model due to its: efficiency on very large tabular datasets, ability to model nonlinear and interaction effects, native handling of class imbalance, and interpretability via SHapley Additive exPlanations (SHAP). A LightGBM classifier was trained on the one-hot encoded feature matrix. Model development included:

**Objective:** binary logistic loss  
**Evaluation metrics:** accuracy, balanced accuracy, precision, recall, F1, ROC-AUC, PR-AUC  
**Cross-validation:** 5-fold CV to estimate generalization performance  
**Hyperparameters:** tuned learning rate, max depth, number of leaves, number of trees  
**Imbalance correction:** scale_pos_weight, as described above

# Results

### Exploratory Data Analysis (EDA)
The tu dataset included 493,282 participants, with 230,967 males and 262,315 females. The sex-stratified descriptive statistics showed broadly similar age distributions between groups, while race, income, and health insurance patterns exhibited modest variation. Marital status differed by sex, with a higher proportion of males being married while females were more frequently not married or separated. Tobacco-related behaviors demonstrated clear sex-specific patterns: males reported higher rates of currently and ever using smokeless tobacco products and were more likely to have smoked at least 100 cigarettes in their lifetime. Additionally, the mean age of smoking initiation was slightly lower among males. Mortality proportions also varied slightly by sex. Overall, Table 1 summarizes these demographic, socioeconomic, and tobacco-use differences, providing a foundational characterization of the NLMS tobacco use cohort prior to inferential analysis.

```{r, echo=FALSE, out.width="50%"}
knitr::include_graphics("files/descript_table.png")
```
### Logistic Regression
In the model adjusting for demographic and socioeconomic covariates, cigarette smoking status showed a strong graded association with mortality. Compared with never smokers, everyday smokers had more than double the odds of death (OR = 2.19), some-day smokers had 78% higher odds (OR = 1.78), and former smokers had 37% higher odds (OR = 1.37), demonstrating a clear dose–response pattern (Table 2).

When age at smoking initiation was added to the model, the associations shifted. After adjustment, some-day smokers showed approximately 14% lower odds of mortality (OR = 0.86) and former smokers showed approximately 39% lower odds (OR = 0.61) relative to everyday smokers, likely reflecting differences in cumulative smoking duration. Age at smoking initiation was independently associated with mortality, with each additional year of later initiation reducing mortality risk by about 2% (Table 3).

Across both models, age, sex, income, race, and marital status remained strong independent predictors. Age showed the largest effect, with each additional year associated with roughly 9% higher mortality risk. Females had substantially lower mortality (about 36–43% lower), while higher income and being married were consistently protective. Race differences persisted, with several groups showing higher or lower mortality risks relative to the reference category.

Overall, smoking status was a robust predictor of mortality, and incorporating smoking duration through age at initiation refined these associations. These findings suggest that both current smoking behavior and lifetime smoking history contribute meaningfully to mortality risk.

```{r, echo=FALSE, out.width="50%"}
knitr::include_graphics("files/reg_results_table.png")
```

### Machine Learning Predictor
#### Validation Results
Five-fold cross-validation of our LightGBM model performed well, with metrics being outlined in Table 1.
```{r, echo = FALSE}
cv_results <- tribble(
~Metric, ~Mean, ~SD,
"Accuracy", 0.8309, 0.0012,
"Balanced Accuracy", 0.8361, 0.0012,
"Precision", 0.1745, 0.0012,
"Recall", 0.8418, 0.0015,
"F1-score", 0.2891, 0.0017,
"ROC-AUC", 0.9087, 0.0022,
"PR-AUC", 0.3497, 0.0099
)
kable(cv_results, digits=4,
caption="TABLE 1: Five-fold cross-validation results for LightGBM.")
```

#### Testing Results
When classifying the held-out test set, the model's accuracy of 0.8265 and balanced accuracy of 0.8341 indicates a strong predictive performance. The closeness of these values indicates that the class imbalance is being handled well, mitigating over fitting.

Our model achieved a recall of 0.8425, meaning that it correctly identified approximately 84% of all individuals who actually died. In public health and clinical risk prediction settings, this is a critical property since false negatives (missed deaths) are more harmful than false positives. High recall indicates that the model is effective at capturing true high-risk cases, even when they form a very small portion of the population. This sensitivity is especially valuable for early intervention, population screening, and prioritizing individuals for health outreach or monitoring.

The model's precision was relatively low (0.1708), which is expected in datasets where the positive class is extremely rare. In this case, low precision does not indicate model failure; instead, it reflects the inherent difficulty of predicting a rare outcome.

```{r, echo=FALSE}
test_results <- tribble(
~Metric, ~Value,
"Accuracy", 0.8265,
"Balanced Accuracy", 0.8341,
"Precision", 0.1708,
"Recall", 0.8425,
"F1-score", 0.2841,
"ROC-AUC", 0.9070,
"PR-AUC", 0.3563
)
kable(test_results, digits=4,
caption="TABLE 2: Held-out test-set performance for LightGBM.")
```

The model’s ROC-AUC of 0.907 indicates excellent ability to distinguish between individuals who died and those who survived across all possible classification thresholds. Additionally, the PR-AUC of 0.356 is strong given the 4% event rate; random guessing would achieve a PR-AUC near 0.04. This demonstrates that the model is identifying high-risk individuals far better than chance, despite class imbalance.

**FIGURE 1:**  

```{r ROC_curve, echo=FALSE, out.width="50%"}
knitr::include_graphics("figures/roc_lightgbm.png")
```

#### Key Feature Effects
To better understand how the model assigns risk, SHAP dependence analysis was used to examine the effect of the most influential predictors. Age showed a strong, monotonic relationship with mortality risk: SHAP values increased steadily with age, indicating that older individuals contribute substantially more to predicted mortality. This effect was consistent across all income levels. Other influential features included citizenship category, age, sex, smoking, and income-to-poverty ratio, reflecting the combined biological and socioeconomic determinants of mortality. Additional information on these features can be found in ________(GitHub path)_________.

**FIGURE 2:**

```{r shap-age, echo=FALSE, out.width="50%"}
knitr::include_graphics("figures/shap_age.png")
```

# Conclusion

Our LightGBM classifier predicted 5-year mortality among smokers in the NLMS Tobacco Use Cohort, a large and highly imbalanced dataset. After one-hot encoding categorical variables and applying class weighting to address the rarity of death events, the model demonstrated strong performance on both cross-validation and held-out test data. High recall and AUC values indicate that the model effectively identifies individuals at elevated mortality risk, even when the positive class represents only ~4% of the population. Although precision is low, as expected in rare-event prediction, the model’s ability to capture the majority of true deaths makes it suitable for screening and population risk stratification. Overall, LightGBM provides a robust, efficient, and interpretable framework for mortality prediction in large epidemiologic datasets, offering practical utility for public health research and targeted intervention strategies.

# References
[1] Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., ... & Liu, T. Y. (2017). Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural information processing systems, 30.
